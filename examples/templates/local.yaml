---
# Source: vulcan/charts/minio/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: myrelease-minio
  namespace: "ns"
  labels:
    app.kubernetes.io/name: minio
    helm.sh/chart: minio-11.7.4
    app.kubernetes.io/instance: myrelease
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  root-user: "QUtJQUlPU0ZPRE5ON0VYQU1QTEU="
  root-password: "d0phbHJYVXRuRkVNSS9LN01ERU5HL2JQeFJmaUNZRVhBTVBMRUtFWQ=="
  key.json: ""
---
# Source: vulcan/charts/postgresql/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: myrelease-postgresql
  namespace: "ns"
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-11.6.6
    app.kubernetes.io/instance: myrelease
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  postgres-password: "VEJE"
  password: "VEJE"
  # We don't auto-generate LDAP password when it's not provided as we do for other passwords
---
# Source: vulcan/templates/api/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: myrelease-vulcan-api
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: api
type: Opaque
data:
  PG_PASSWORD: "VEJE"
  SECRET_KEY: "VEJEVEJE"
  AWSCATALOGUE_KEY: "a2V5"
---
# Source: vulcan/templates/crontinuous/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: myrelease-vulcan-crontinuous
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crontinuous
type: Opaque
data:
  VULCAN_TOKEN: "VEJEVEJEVEJE"
---
# Source: vulcan/templates/dogstatsd-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: myrelease-vulcan-dogstatsd
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: dogstatsd
type: Opaque
data:
  DD_API_KEY: "VEJE"
---
# Source: vulcan/templates/metrics/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: myrelease-vulcan-metrics
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: metrics
type: Opaque
data:
  DEVHOSE_TOKEN: "dG9rZW4="
  VULCAN_API_TOKEN: "dG9rZW4="
---
# Source: vulcan/templates/persistence/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: myrelease-vulcan-persistence
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: persistence
type: Opaque
data:
  POSTGRES_PASSWORD: "VEJE"
  SECRET_KEY_BASE: "VEJEVEJE"
---
# Source: vulcan/templates/reportsgenerator/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: myrelease-vulcan-reportsgenerator
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: reportsgenerator
type: Opaque
data:
  PG_PASSWORD: "VEJE"
---
# Source: vulcan/templates/scanengine/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: myrelease-vulcan-scanengine
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: scanengine
type: Opaque
data:
  PG_PASSWORD: "VEJE"
---
# Source: vulcan/templates/stream/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: myrelease-vulcan-stream
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: stream
type: Opaque
data:
  REDIS_PWD: ""
---
# Source: vulcan/templates/vulndb/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: myrelease-vulcan-vulndb
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: vulndb
type: Opaque
data:
  PG_PASSWORD: "VEJE"
---
# Source: vulcan/templates/vulndbapi/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: myrelease-vulcan-vulndbapi
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: vulndbapi
type: Opaque
data:
  PG_PASSWORD: "VEJE"
---
# Source: vulcan/charts/postgresql/templates/primary/initialization-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myrelease-postgresql-init-scripts
  namespace: "ns"
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-11.6.6
    app.kubernetes.io/instance: myrelease
    app.kubernetes.io/managed-by: Helm
data:
  initial-dbs.sql: |
    CREATE DATABASE api;
    CREATE DATABASE scanengine;
    CREATE DATABASE reportsgenerator;
    CREATE DATABASE vulnerabilitydb;
---
# Source: vulcan/charts/redis/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myrelease-redis-configuration
  namespace: "ns"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-16.12.1
    app.kubernetes.io/instance: myrelease
    app.kubernetes.io/managed-by: Helm
data:
  redis.conf: |-
    # User-supplied common configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
    # End of common configuration
  master.conf: |-
    dir /data
    # User-supplied master configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of master configuration
  replica.conf: |-
    dir /data
    slave-read-only yes
    # User-supplied replica configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of replica configuration
---
# Source: vulcan/charts/redis/templates/health-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myrelease-redis-health
  namespace: "ns"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-16.12.1
    app.kubernetes.io/instance: myrelease
    app.kubernetes.io/managed-by: Helm
data:
  ping_readiness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ] && [ "$responseFirstWord" != "MASTERDOWN" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
---
# Source: vulcan/charts/redis/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myrelease-redis-scripts
  namespace: "ns"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-16.12.1
    app.kubernetes.io/instance: myrelease
    app.kubernetes.io/managed-by: Helm
data:
  start-master.sh: |
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
    fi
    if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--protected-mode" "no")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
    exec redis-server "${ARGS[@]}"
---
# Source: vulcan/templates/api/deployment.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myrelease-vulcan-api-proxy
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: api
data:
  haproxy.cfg: |
    global
      daemon
      maxconn 64
      log stdout format raw daemon

    defaults
      mode http
      timeout connect 5s
      timeout client 25s
      timeout server 25s
      timeout tunnel 3600s
      option  http-server-close

    frontend http
      bind *:9090
      log global
      option httplog clf
      http-request capture req.hdr(Host) len 50
      http-request capture req.hdr(User-Agent) len 100

      default_backend app

    backend app
      server app 127.0.0.1:8080

    frontend stats
      bind *:9101
      option http-use-htx
      http-request use-service prometheus-exporter if { path /metrics }
      monitor-uri /healthz
---
# Source: vulcan/templates/crontinuous/deployment.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myrelease-vulcan-crontinuous-proxy
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crontinuous
data:
  haproxy.cfg: |
    global
      daemon
      maxconn 64
      log stdout format raw daemon

    defaults
      mode http
      timeout connect 5s
      timeout client 25s
      timeout server 25s
      timeout tunnel 3600s
      option  http-server-close

    frontend http
      bind *:9090
      log global
      option httplog clf
      http-request capture req.hdr(Host) len 50
      http-request capture req.hdr(User-Agent) len 100

      default_backend app

    backend app
      server app 127.0.0.1:8080

    frontend stats
      bind *:9101
      option http-use-htx
      http-request use-service prometheus-exporter if { path /metrics }
      monitor-uri /healthz
---
# Source: vulcan/templates/goaws/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myrelease-vulcan-goaws
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: goaws
data:
  goaws.yaml: |
    Local:
      Host: myrelease-vulcan-goaws
      Port: 8080
      AccountId: "012345678900"
      LogToFile: false
      QueueAttributeDefaults:
        VisibilityTimeout: 30
        ReceiveMessageWaitTimeSeconds: 0
      Queues:
        - Name: VulcanK8SAPIScans
        - Name: VulcanK8SMetricsChecks
        - Name: VulcanK8SMetricsFindings
        - Name: VulcanK8SMetricsScans
        - Name: VulcanK8SReportsGenerator
        - Name: VulcanK8SScanEngineCheckStatus
        - Name: VulcanK8SV2ChecksGeneric
        - Name: VulcanK8SVulnDBChecks
      Topics:
        - Name: VulcanK8SChecks
          Subscriptions:
            - QueueName: VulcanK8SMetricsChecks
              Raw: true
            - QueueName: VulcanK8SVulnDBChecks
              Raw: true
        - Name: VulcanK8SScans
          Subscriptions:
            - QueueName: VulcanK8SAPIScans
              Raw: true
            - QueueName: VulcanK8SMetricsScans
              Raw: true
        - Name: VulcanK8SReportsGen
          Subscriptions:
            - QueueName: VulcanK8SReportsGenerator
              Raw: true
        - Name: VulcanK8SVulnDBVulns
          Subscriptions:
            - QueueName: VulcanK8SMetricsFindings
              Raw: true
      RandomLatency:
        Min: 0
        Max: 0
---
# Source: vulcan/templates/insights/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myrelease-vulcan-insights-proxy
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: insights
data:
  haproxy.cfg: |
    global
      daemon
      maxconn 64
      log stdout format raw daemon
  
    defaults
      mode http
      timeout connect 5s
      timeout client 25s
      timeout server 25s
      timeout tunnel 3600s
      option  http-server-close
    cache small
      total-max-size 64     # mb
      max-age 240           # seconds
  
    frontend http
      bind *:9090
      log global
      option httplog clf
      http-request cache-use small
      http-response cache-store small
      http-request capture req.hdr(Host) len 50
      http-request capture req.hdr(User-Agent) len 100
      default_backend private
      use_backend public if { path -i -m beg /public }
  
    backend private
      server app 127.0.0.1:8080
  
    backend public
      server app 127.0.0.1:8081
  
    frontend stats
      bind *:9101
      option http-use-htx
      http-request use-service prometheus-exporter if { path /metrics }
      monitor-uri /healthz
---
# Source: vulcan/templates/persistence/deployment.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myrelease-vulcan-persistence-proxy
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: persistence
data:
  haproxy.cfg: |
    global
      daemon
      maxconn 64
      log stdout format raw daemon

    defaults
      mode http
      timeout connect 5s
      timeout client 25s
      timeout server 25s
      timeout tunnel 3600s
      option  http-server-close

    frontend http
      bind *:9090
      log global
      option httplog clf
      http-request capture req.hdr(Host) len 50
      http-request capture req.hdr(User-Agent) len 100

      default_backend app

    backend app
      server app 127.0.0.1:8080

    frontend stats
      bind *:9101
      option http-use-htx
      http-request use-service prometheus-exporter if { path /metrics }
      monitor-uri /healthz
---
# Source: vulcan/templates/reportsgenerator/deployment.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myrelease-vulcan-reportsgenerator-proxy
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: reportsgenerator
data:
  haproxy.cfg: |
    global
      daemon
      maxconn 64
      log stdout format raw daemon

    defaults
      mode http
      timeout connect 5s
      timeout client 25s
      timeout server 25s
      timeout tunnel 3600s
      option  http-server-close

    frontend http
      bind *:9090
      log global
      option httplog clf
      http-request capture req.hdr(Host) len 50
      http-request capture req.hdr(User-Agent) len 100

      default_backend app

    backend app
      server app 127.0.0.1:8080

    frontend stats
      bind *:9101
      option http-use-htx
      http-request use-service prometheus-exporter if { path /metrics }
      monitor-uri /healthz
---
# Source: vulcan/templates/results/deployment.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myrelease-vulcan-results-proxy
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: results
data:
  haproxy.cfg: |
    global
      daemon
      maxconn 64
      log stdout format raw daemon

    defaults
      mode http
      timeout connect 5s
      timeout client 25s
      timeout server 25s
      timeout tunnel 3600s
      option  http-server-close

    frontend http
      bind *:9090
      log global
      option httplog clf
      http-request capture req.hdr(Host) len 50
      http-request capture req.hdr(User-Agent) len 100

      default_backend app

    backend app
      server app 127.0.0.1:8080

    frontend stats
      bind *:9101
      option http-use-htx
      http-request use-service prometheus-exporter if { path /metrics }
      monitor-uri /healthz
---
# Source: vulcan/templates/scanengine/deployment.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myrelease-vulcan-scanengine-proxy
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: scanengine
data:
  haproxy.cfg: |
    global
      daemon
      maxconn 64
      log stdout format raw daemon

    defaults
      mode http
      timeout connect 5s
      timeout client 25s
      timeout server 25s
      timeout tunnel 3600s
      option  http-server-close

    frontend http
      bind *:9090
      log global
      option httplog clf
      http-request capture req.hdr(Host) len 50
      http-request capture req.hdr(User-Agent) len 100

      default_backend app

    backend app
      server app 127.0.0.1:8080

    frontend stats
      bind *:9101
      option http-use-htx
      http-request use-service prometheus-exporter if { path /metrics }
      monitor-uri /healthz
---
# Source: vulcan/templates/stream/deployment.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myrelease-vulcan-stream-proxy
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: stream
data:
  haproxy.cfg: |
    global
      daemon
      maxconn 64
      log stdout format raw daemon

    defaults
      mode http
      timeout connect 5s
      timeout client 25s
      timeout server 25s
      timeout tunnel 3600s
      option  http-server-close

    frontend http
      bind *:9090
      log global
      option httplog clf
      http-request capture req.hdr(Host) len 50
      http-request capture req.hdr(User-Agent) len 100

      default_backend app

    backend app
      server app 127.0.0.1:8080

    frontend stats
      bind *:9101
      option http-use-htx
      http-request use-service prometheus-exporter if { path /metrics }
      monitor-uri /healthz
---
# Source: vulcan/templates/ui/deployment.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myrelease-vulcan-ui-proxy
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ui
data:
  haproxy.cfg: |
    global
      daemon
      maxconn 64
      log stdout format raw daemon

    defaults
      mode http
      timeout connect 5s
      timeout client 25s
      timeout server 25s
      timeout tunnel 3600s
      option  http-server-close

    frontend http
      bind *:9090
      log global
      option httplog clf
      http-request capture req.hdr(Host) len 50
      http-request capture req.hdr(User-Agent) len 100

      default_backend app

    backend app
      server app 127.0.0.1:8080

    frontend stats
      bind *:9101
      option http-use-htx
      http-request use-service prometheus-exporter if { path /metrics }
      monitor-uri /healthz
---
# Source: vulcan/templates/vulndbapi/deployment.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myrelease-vulcan-vulndbapi-proxy
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: vulndbapi
data:
  haproxy.cfg: |
    global
      daemon
      maxconn 64
      log stdout format raw daemon

    defaults
      mode http
      timeout connect 5s
      timeout client 25s
      timeout server 25s
      timeout tunnel 3600s
      option  http-server-close

    frontend http
      bind *:9090
      log global
      option httplog clf
      http-request capture req.hdr(Host) len 50
      http-request capture req.hdr(User-Agent) len 100

      default_backend app

    backend app
      server app 127.0.0.1:8080

    frontend stats
      bind *:9101
      option http-use-htx
      http-request use-service prometheus-exporter if { path /metrics }
      monitor-uri /healthz
---
# Source: vulcan/charts/minio/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myrelease-minio
  namespace: "ns"
  labels:
    app.kubernetes.io/name: minio
    helm.sh/chart: minio-11.7.4
    app.kubernetes.io/instance: myrelease
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: minio-api
      port: 80
      targetPort: minio-api
      nodePort: null
    - name: minio-console
      port: 9001
      targetPort: minio-console
      nodePort: null
  selector:
    app.kubernetes.io/name: minio
    app.kubernetes.io/instance: myrelease
---
# Source: vulcan/charts/postgresql/templates/primary/metrics-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: myrelease-postgresql-metrics
  namespace: "ns"
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-11.6.6
    app.kubernetes.io/instance: myrelease
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
  annotations:
    prometheus.io/port: '9187'
    prometheus.io/scrape: "true"
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: http-metrics
      port: 9187
      targetPort: http-metrics
  selector:
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/instance: myrelease
    app.kubernetes.io/component: primary
---
# Source: vulcan/charts/postgresql/templates/primary/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: myrelease-postgresql-hl
  namespace: "ns"
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-11.6.6
    app.kubernetes.io/instance: myrelease
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
    # Use this annotation in addition to the actual publishNotReadyAddresses
    # field below because the annotation will stop being respected soon but the
    # field is broken in some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  # We want all pods in the StatefulSet to have their addresses published for
  # the sake of the other Postgresql pods even before they're ready, since they
  # have to be able to talk to each other in order to become ready.
  publishNotReadyAddresses: true
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/instance: myrelease
    app.kubernetes.io/component: primary
---
# Source: vulcan/charts/postgresql/templates/primary/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: myrelease-postgresql
  namespace: "ns"
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-11.6.6
    app.kubernetes.io/instance: myrelease
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
  annotations:
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
      nodePort: null
  selector:
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/instance: myrelease
    app.kubernetes.io/component: primary
---
# Source: vulcan/charts/postgresql/templates/read/metrics-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: myrelease-postgresql-read-metrics
  namespace: "ns"
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-11.6.6
    app.kubernetes.io/instance: myrelease
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
  annotations:
    prometheus.io/port: '9187'
    prometheus.io/scrape: "true"
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: http-metrics
      port: 9187
      targetPort: http-metrics
  selector:
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/instance: myrelease
    app.kubernetes.io/component: read
---
# Source: vulcan/charts/redis/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: myrelease-redis-headless
  namespace: "ns"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-16.12.1
    app.kubernetes.io/instance: myrelease
    app.kubernetes.io/managed-by: Helm
  annotations:
    
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: myrelease
---
# Source: vulcan/charts/redis/templates/master/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myrelease-redis-master
  namespace: "ns"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-16.12.1
    app.kubernetes.io/instance: myrelease
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: master
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
      nodePort: null
  selector:
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: myrelease
    app.kubernetes.io/component: master
---
# Source: vulcan/templates/api/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myrelease-vulcan-api
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: api
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/name: api
---
# Source: vulcan/templates/crontinuous/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myrelease-vulcan-crontinuous
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crontinuous
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/name: crontinuous
---
# Source: vulcan/templates/goaws/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myrelease-vulcan-goaws
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: goaws
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/name: goaws
---
# Source: vulcan/templates/insights/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myrelease-vulcan-insights
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: insights
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/name: insights
---
# Source: vulcan/templates/persistence/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myrelease-vulcan-persistence
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: persistence
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/name: persistence
---
# Source: vulcan/templates/reportsgenerator/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myrelease-vulcan-reportsgenerator
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: reportsgenerator
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/name: reportsgenerator
---
# Source: vulcan/templates/results/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myrelease-vulcan-results
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: results
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/name: results
---
# Source: vulcan/templates/scanengine/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myrelease-vulcan-scanengine
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: scanengine
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/name: scanengine
---
# Source: vulcan/templates/stream/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myrelease-vulcan-stream
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: stream
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/name: stream
---
# Source: vulcan/templates/ui/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myrelease-vulcan-ui
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ui
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/name: ui
---
# Source: vulcan/templates/vulndbapi/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myrelease-vulcan-vulndbapi
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: vulndbapi
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/name: vulndbapi
---
# Source: vulcan/charts/minio/templates/standalone/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myrelease-minio
  namespace: "ns"
  labels:
    app.kubernetes.io/name: minio
    helm.sh/chart: minio-11.7.4
    app.kubernetes.io/instance: myrelease
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: minio
      app.kubernetes.io/instance: myrelease
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: minio
        helm.sh/chart: minio-11.7.4
        app.kubernetes.io/instance: myrelease
        app.kubernetes.io/managed-by: Helm
      annotations:
        checksum/credentials-secret: d8a5fa3168b7010804804f8669338bfac47e3b645c0a9884705ccf80345d32c9
    spec:
      
      serviceAccountName: default
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: minio
                    app.kubernetes.io/instance: myrelease
                namespaces:
                  - "ns"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      containers:
        - name: minio
          image: docker.io/bitnami/minio:2022.6.11-debian-11-r0
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MINIO_SCHEME
              value: "http"
            - name: MINIO_FORCE_NEW_KEYS
              value: "no"
            - name: MINIO_ROOT_USER
              valueFrom:
                secretKeyRef:
                  name: myrelease-minio
                  key: root-user
            - name: MINIO_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: myrelease-minio
                  key: root-password
            - name: MINIO_DEFAULT_BUCKETS
              value: reports,logs,scans,insights,public-insights,crontinuous
            - name: MINIO_BROWSER
              value: "on"
            - name: MINIO_PROMETHEUS_AUTH_TYPE
              value: "public"
            - name: MINIO_CONSOLE_PORT_NUMBER
              value: "9001"
          envFrom:
          ports:
            - name: minio-api
              containerPort: 9000
              protocol: TCP
            - name: minio-console
              containerPort: 9001
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /minio/health/live
              port: minio-api
              scheme: "HTTP"
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
          readinessProbe:
            tcpSocket:
              port: minio-api
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 5
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: data
              mountPath: /data
      volumes:
        - name: data
          emptyDir: {}
---
# Source: vulcan/templates/api/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myrelease-vulcan-api
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: api
spec:
  selector:
    matchLabels:
      app.kubernetes.io/instance: vulcan
      app.kubernetes.io/name: api
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: vulcan
        app.kubernetes.io/name: api
      annotations:
        checksum/secrets: 334c1bcc1552336e9d4df7f38207ac0f0ed745e2609adb8897c924f761fef0e0
        checksum/config-proxy: 50b289164aaee5d9740e0a8a1ed839f1e6933e1ac12d57ceb0ae35beaccdf5ce
        prometheus.io/scrape: 'true'
        prometheus.io/port: '9101'
    spec:
      initContainers:
        - name: waitfordb
          image: "busybox:1.35.0"
          imagePullPolicy: Always
          command: ['sh', '-c', 'until nc -z "$PGHOST" "$PGPORT"; do echo WaitingDB && sleep 5; done;']
          env:
          - name: PGHOST
            value: "myrelease-postgresql"
          - name: PGPORT
            value: "5432"
      containers:
        
        - name: dogstatsd
          image: "datadog/dogstatsd:7.37.1"
          envFrom:
          - secretRef:
              name: myrelease-vulcan-dogstatsd
          ports:
            - containerPort: 8125
              name: dogstatsd
              protocol: UDP
        - name: proxy
          image: "haproxy:2.4.17-alpine3.16"
          imagePullPolicy: Always
          ports:
            - name: http
              containerPort: 9090
            - name: metrics
              containerPort: 9101
          volumeMounts:
          - mountPath: /usr/local/etc/haproxy
            readOnly: true
            name: config-proxy
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh","-c","sleep 30;"]
        - name: api
          
          image: "adevinta/vulcan-api:1.0"
          imagePullPolicy: Always
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh","-c","sleep 30;"]
          livenessProbe:
            httpGet:
              path: /api/v1/healthcheck
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 10
          readinessProbe:
            httpGet:
              path: /api/v1/healthcheck
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 5
          env:
          - name: PORT
            value: "8080"
          - name: PG_HOST
            value: "myrelease-postgresql"
          - name: PG_NAME
            value: "api"
          - name: PG_USER
            value: "postgres"
          - name: PG_PORT
            value: "5432"
          - name: PG_SSLMODE
            value: "disable"
          - name: PG_CA_B64
            value: ""
          - name: LOG_LEVEL
            value: "INFO"
          - name: COOKIE_DOMAIN
            value: "vulcan.local"
          - name: SAML_MEATADATA
            value: "https://okta/app/TBD/sso/saml/metadata"
          - name: SAML_ISSUER
            value: "http://okta/TBD"
          - name: SAML_CALLBACK
            value: "https://www.vulcan.local/api/v1/login/callback"
          - name: SAML_TRUSTED_DOMAINS
            value: "[\"www.vulcan.local\"]"
          - name: DEFAULT_OWNERS
            value: "[]"
          - name: SCANENGINE_URL
            value: "http://myrelease-vulcan-scanengine/v1/"
          - name: SCHEDULER_URL
            value: "http://myrelease-vulcan-crontinuous/"
          - name: SQS_QUEUE_ARN
            value: "arn:aws:sqs:local:012345678900:VulcanK8SAPIScans"
          - name: REPORTS_SNS_ARN
            value: "arn:aws:sns:local:012345678900:VulcanK8SReportsGen"
          - name: REPORTS_API_URL
            value: "http://myrelease-vulcan-reportsgenerator/"
          - name: SCAN_REDIRECT_URL
            value: 
          - name: VULCAN_UI_URL
            value: 
          - name: PERSISTENCE_HOST
            value: "myrelease-vulcan-persistence"
          - name: VULNERABILITYDB_URL
            value: "http://myrelease-vulcan-vulndbapi/"
          - name: AWSCATALOGUE_KIND
            value: "None"
          - name: AWSCATALOGUE_URL
            value: "http://catalogue.example.com"
          - name: AWSCATALOGUE_RETRIES
            value: "1"
          - name: AWSCATALOGUE_RETRY_INTERVAL
            value: "2"
          - name: "GPC_1_NAME"
            value: "web-scanning-global"
          - name: "GPC_1_ALLOWED_ASSETTYPES"
            value: "[]"
          - name: "GPC_1_BLOCKED_ASSETTYPES"
            value: "[]"
          - name: "GPC_1_ALLOWED_CHECKS"
            value: "[\"vulcan-zap\"]"
          - name: "GPC_1_BLOCKED_CHECKS"
            value: "[]"
          - name: "GPC_1_EXCLUDING_SUFFIXES"
            value: "[\"experimental\"]"
          - name: AWS_SNS_ENDPOINT
            value: "http://myrelease-vulcan-goaws"
          - name: AWS_SQS_ENDPOINT
            value: "http://myrelease-vulcan-goaws"
          - name: AWS_S3_ENDPOINT
            value: "http://myrelease-minio"
          - name: PATH_STYLE
            value: "true"
          - name: AWS_S3_REGION
            value: "local"
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: myrelease-minio
                key: root-user
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: myrelease-minio
                key: root-password
          - name: DOGSTATSD_ENABLED
            value: "true"
          - name: DOGSTATSD_HOST
            value: "localhost"
          - name: DOGSTATSD_PORT
            value: "8125"
          envFrom:
          - secretRef:
              name: myrelease-vulcan-api
          ports:
            - name: app
              containerPort: 8080
              protocol: TCP
      volumes:
      - name: config-proxy
        configMap:
          name: myrelease-vulcan-api-proxy
---
# Source: vulcan/templates/crontinuous/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myrelease-vulcan-crontinuous
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: crontinuous
spec:
  selector:
    matchLabels:
      app.kubernetes.io/instance: vulcan
      app.kubernetes.io/name: crontinuous
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: vulcan
        app.kubernetes.io/name: crontinuous
      annotations:
        checksum/secrets: bf354bf4627fa6b7607813eb82095cb39012db00cd08277e49a6951b1f365e74
        checksum/config-proxy: 61779492504d095c11f47249227b0a18234909a250822d148e2859fba85bae71
        prometheus.io/scrape: 'true'
        prometheus.io/port: '9101'
    spec:
      containers:
        
        - name: proxy
          image: "haproxy:2.4.17-alpine3.16"
          imagePullPolicy: Always
          ports:
            - name: http
              containerPort: 9090
            - name: metrics
              containerPort: 9101
          volumeMounts:
          - mountPath: /usr/local/etc/haproxy
            readOnly: true
            name: config-proxy
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh","-c","sleep 30;"]
        - name: crontinuous
          
          image: "adevinta/vulcan-crontinuous:1.0"
          imagePullPolicy: Always
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh","-c","sleep 30;"]
          livenessProbe:
            httpGet:
              path: /healthcheck
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 10
          readinessProbe:
            httpGet:
              path: /healthcheck
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 5
          env:
          - name: PORT
            value: "8080"
          - name: AWS_REGION
            value: local
          - name: CRONTINUOUS_BUCKET
            value: crontinuous
          - name: VULCAN_API
            value: http://myrelease-vulcan-api/api
          - name: VULCAN_USER
            value: tbd
          - name: ENABLE_TEAMS_WHITELIST_SCAN
            value: "false"
          - name: TEAMS_WHITELIST_SCAN
            value: "[\"team1\", \"team2\"]"
          - name: ENABLE_TEAMS_WHITELIST_REPORT
            value: "false"
          - name: TEAMS_WHITELIST_REPORT
            value: "[\"team3\"]"
          
          - name: AWS_S3_ENDPOINT
            value: "http://myrelease-minio"
          - name: PATH_STYLE
            value: "true"
          - name: AWS_S3_REGION
            value: "local"
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: myrelease-minio
                key: root-user
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: myrelease-minio
                key: root-password
          
          envFrom:
          - secretRef:
              name: myrelease-vulcan-crontinuous
          ports:
            - name: app
              containerPort: 8080
              protocol: TCP
      volumes:
      - name: config-proxy
        configMap:
          name: myrelease-vulcan-crontinuous-proxy
---
# Source: vulcan/templates/goaws/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myrelease-vulcan-goaws
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: goaws
spec:
  selector:
    matchLabels:
      app.kubernetes.io/instance: vulcan
      app.kubernetes.io/name: goaws
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: vulcan
        app.kubernetes.io/name: goaws
      annotations:
        checksum/config: f8cd2f9bcf237b80cd8e074c4bc5d84aebccfbf9ade3caf600b2338ad3adf9b5
    spec:
      containers:
      - name: goaws
        image: "pafortin/goaws:v0.3.1"
        imagePullPolicy: Always
        ports:
          - name: http
            containerPort: 8080
            protocol: TCP
        volumeMounts:
        - name: goaws-config
          mountPath: /conf
      volumes:
      - name: goaws-config
        configMap:
          name: myrelease-vulcan-goaws
---
# Source: vulcan/templates/insights/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myrelease-vulcan-insights
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: insights
spec:
  selector:
    matchLabels:
      app.kubernetes.io/instance: vulcan
      app.kubernetes.io/name: insights
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: vulcan
        app.kubernetes.io/name: insights
      annotations:
        checksum/config: 81a6311e0ee5af8f9be60036d726af473fb4de232b8b7afe99ea6525ded53d31
        prometheus.io/scrape: 'true'
        prometheus.io/port: '9101'
    spec:
      containers:
        
        - name: proxy
          image: "haproxy:2.4.17-alpine3.16"
          imagePullPolicy: Always
          ports:
            - name: http
              containerPort: 9090
            - name: metrics
              containerPort: 9101
          volumeMounts:
          - mountPath: /usr/local/etc/haproxy
            readOnly: true
            name: config-proxy
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh","-c","sleep 30;"]
        - name: insights-private
          
          image: "pottava/s3-proxy:2.0"
          imagePullPolicy: Always
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh","-c","sleep 30;"]
          livenessProbe:
            httpGet:
              path: /healthcheck
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 10
          readinessProbe:
            httpGet:
              path: /healthcheck
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 5
          env:
            - name: AWS_REGION
              value: "local"
            - name: ACCESS_LOG
              value: "false"
            - name: AWS_S3_BUCKET
              value: "insights"
            - name: STRIP_PATH
              value: ""
            - name: HEALTHCHECK_PATH
              value: "/healthcheck"
            - name: APP_PORT
              value: "8080"
          ports:
            - name: private
              containerPort: 8080
              protocol: TCP
        - name: insights-public
          
          image: "pottava/s3-proxy:2.0"
          imagePullPolicy: Always
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh","-c","sleep 30;"]
          livenessProbe:
            httpGet:
              path: /healthcheck
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 10
          readinessProbe:
            httpGet:
              path: /healthcheck
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 5
          env:
            - name: AWS_REGION
              value: "local"
            - name: ACCESS_LOG
              value: "false"
            - name: AWS_S3_BUCKET
              value: "public-insights"
            - name: STRIP_PATH
              value: "/public"
            - name: HEALTHCHECK_PATH
              value: "/healthcheck"
            - name: APP_PORT
              value: "8081"
          ports:
            - name: private
              containerPort: 8081
              protocol: TCP
      volumes:
      - name: config-proxy
        configMap:
          name: myrelease-vulcan-insights-proxy
---
# Source: vulcan/templates/metrics/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myrelease-vulcan-metrics
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: metrics
spec:
  selector:
    matchLabels:
      app.kubernetes.io/instance: vulcan
      app.kubernetes.io/name: metrics
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: vulcan
        app.kubernetes.io/name: metrics
      annotations:
        checksum/secrets: b0d413a0b3902a84c0f51e59f152d29668f629dd011d9d8462b70fd2cbd5a1c3
        
    spec:
      containers:
        
        - name: dogstatsd
          image: "datadog/dogstatsd:7.37.1"
          envFrom:
          - secretRef:
              name: myrelease-vulcan-dogstatsd
          ports:
            - containerPort: 8125
              name: dogstatsd
              protocol: UDP
        - name: redis
          image: "bitnami/redis:6.2.7"
          env:
          - name: ALLOW_EMPTY_PASSWORD
            value: "yes"
          ports:
          - containerPort: 6379
            name: redis
            protocol: TCP
        - name: metrics
          
          image: "containers.mpi-internal.com/spt-security/vulcan-metrics:1.0"
          imagePullPolicy: Always
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh","-c","sleep 30;"]
          env:
          - name: LOG_LEVEL
            value: "warn"
          - name: SQS_POLLING_INTERVAL
            value: "10"
          - name: CHECKS_SQS_QUEUE_ARN
            value: "arn:aws:sqs:local:012345678900:VulcanK8SMetricsChecks"
          - name: SCANS_SQS_QUEUE_ARN
            value: "arn:aws:sqs:local:012345678900:VulcanK8SMetricsScans"
          - name: FINDINGS_SQS_QUEUE_ARN
            value: "arn:aws:sqs:local:012345678900:VulcanK8SMetricsFindings"
          - name: RESULTS_HOST
            value: "myrelease-vulcan-results"
          - name: RESULTS_SCHEME
            value: "http"
          - name: DEVHOSE_URL
            value: "http://devhose/devhose"
          - name: DEVHOSE_TENANT
            value: "tbd"
          - name: DEVHOSE_METRICS_SOURCE
            value: "tbd"
          - name: DEVHOSE_FINDINGS_SOURCE
            value: "tbd"
          - name: REDIS_ADDR
            value: "localhost:6379"
          - name: VULCAN_API
            value: http://myrelease-vulcan-api/api
          - name: VULCAN_API_EXTERNAL
            value: 
          
          - name: AWS_SQS_ENDPOINT
            value: "http://myrelease-vulcan-goaws"
          - name: AWS_ACCESS_KEY_ID
            value: ANYVALUE
          - name: AWS_SECRET_ACCESS_KEY
            value: ANYVALUE
          - name: DOGSTATSD_ENABLED
            value: "true"
          - name: DOGSTATSD_HOST
            value: "localhost"
          - name: DOGSTATSD_PORT
            value: "8125"
          envFrom:
          - secretRef:
              name: myrelease-vulcan-metrics
      volumes:
---
# Source: vulcan/templates/persistence/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myrelease-vulcan-persistence
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: persistence
spec:
  selector:
    matchLabels:
      app.kubernetes.io/instance: vulcan
      app.kubernetes.io/name: persistence
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: vulcan
        app.kubernetes.io/name: persistence
      annotations:
        checksum/secrets: f634e1e1ecf85a98c0459e4ec3b59cf5ea5006ae7d6d377cd9a474ec46fde4a7
        checksum/config-proxy: 6c67bb6c5068c8adb7e999168325747a1abf25d546eaa9ab3bccefe9eb7fd3d7
        prometheus.io/scrape: 'true'
        prometheus.io/port: '9101'
    spec:
      initContainers:
        - name: waitfordb
          image: "busybox:1.35.0"
          imagePullPolicy: Always
          command: ['sh', '-c', 'until nc -z "$PGHOST" "$PGPORT"; do echo WaitingDB && sleep 5; done;']
          env:
          - name: PGHOST
            value: "myrelease-postgresql"
          - name: PGPORT
            value: "5432"
      containers:
        
        - name: dogstatsd
          image: "datadog/dogstatsd:7.37.1"
          envFrom:
          - secretRef:
              name: myrelease-vulcan-dogstatsd
          ports:
            - containerPort: 8125
              name: dogstatsd
              protocol: UDP
        - name: proxy
          image: "haproxy:2.4.17-alpine3.16"
          imagePullPolicy: Always
          ports:
            - name: http
              containerPort: 9090
            - name: metrics
              containerPort: 9101
          volumeMounts:
          - mountPath: /usr/local/etc/haproxy
            readOnly: true
            name: config-proxy
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh","-c","sleep 30;"]
        - name: persistence
          
          image: "adevinta/vulcan-persistence:1.0"
          imagePullPolicy: Always
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh","-c","sleep 30;"]
          livenessProbe:
            httpGet:
              path: /status
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 10
          readinessProbe:
            httpGet:
              path: /status
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 5
          env:
          - name: PORT
            value: "8080"
          - name: POSTGRES_HOST
            value: "myrelease-postgresql"
          - name: POSTGRES_DB
            value: "persistence"
          - name: POSTGRES_USER
            value: "postgres"
          - name: POSTGRES_PORT
            value: "5432"
          - name: POSTGRES_SSLMODE
            value: "disable"
          - name: POSTGRES_CA_B64
            value: ""
          - name: LOG_LEVEL
            value: "warn"
          - name: RAILS_MAX_THREADS
            value: "4"
          - name: AWS_SNS_ENDPOINT
            value: "http://myrelease-vulcan-goaws"
          - name: AWS_SQS_ENDPOINT
            value: "http://myrelease-vulcan-goaws"
          - name: AWS_S3_ENDPOINT
            value: "http://myrelease-minio"
          - name: PATH_STYLE
            value: "true"
          - name: AWS_S3_REGION
            value: "local"
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: myrelease-minio
                key: root-user
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: myrelease-minio
                key: root-password
          - name: DOGSTATSD_ENABLED
            value: "true"
          - name: DOGSTATSD_HOST
            value: "localhost"
          - name: DOGSTATSD_PORT
            value: "8125"
          envFrom:
          - secretRef:
              name: myrelease-vulcan-persistence
          ports:
            - name: app
              containerPort: 8080
              protocol: TCP
      volumes:
      - name: config-proxy
        configMap:
          name: myrelease-vulcan-persistence-proxy
---
# Source: vulcan/templates/reportsgenerator/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myrelease-vulcan-reportsgenerator
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: reportsgenerator
spec:
  selector:
    matchLabels:
      app.kubernetes.io/instance: vulcan
      app.kubernetes.io/name: reportsgenerator
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: vulcan
        app.kubernetes.io/name: reportsgenerator
      annotations:
        checksum/secrets: ff76b6caa4d4af25d5701d0fe380652c74ea9038f2eb3644af242192d8f68fc0
        checksum/config-proxy: 26dce9f124ceb377008594f444c91a10fe5a68b2ba04d8e3be5eda4d8812aa24
        prometheus.io/scrape: 'true'
        prometheus.io/port: '9101'
    spec:
      initContainers:
        - name: waitfordb
          image: "busybox:1.35.0"
          imagePullPolicy: Always
          command: ['sh', '-c', 'until nc -z "$PGHOST" "$PGPORT"; do echo WaitingDB && sleep 5; done;']
          env:
          - name: PGHOST
            value: "myrelease-postgresql"
          - name: PGPORT
            value: "5432"
      containers:
        
        - name: dogstatsd
          image: "datadog/dogstatsd:7.37.1"
          envFrom:
          - secretRef:
              name: myrelease-vulcan-dogstatsd
          ports:
            - containerPort: 8125
              name: dogstatsd
              protocol: UDP
        - name: proxy
          image: "haproxy:2.4.17-alpine3.16"
          imagePullPolicy: Always
          ports:
            - name: http
              containerPort: 9090
            - name: metrics
              containerPort: 9101
          volumeMounts:
          - mountPath: /usr/local/etc/haproxy
            readOnly: true
            name: config-proxy
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh","-c","sleep 30;"]
        - name: reportsgenerator
          
          image: "adevinta/vulcan-reports-generator:1.0"
          imagePullPolicy: Always
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh","-c","sleep 30;"]
          livenessProbe:
            httpGet:
              path: /healthcheck
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 10
          readinessProbe:
            httpGet:
              path: /healthcheck
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 5
          env:
          - name: PORT
            value: "8080"
          - name: PG_HOST
            value: "myrelease-postgresql"
          - name: PG_NAME
            value: "reportsgenerator"
          - name: PG_USER
            value: "postgres"
          - name: PG_PORT
            value: "5432"
          - name: PG_SSLMODE
            value: "disable"
          - name: PG_CA_B64
            value: ""
          - name: LOG_LEVEL
            value: "error"
          - name: SQS_QUEUE_ARN
            value: "arn:aws:sqs:local:012345678900:VulcanK8SReportsGenerator"
          - name: SES_REGION
            value: "local"
          - name: SES_FROM
            value: "tbd@tbd.com"
          - name: SES_CC
            value: "[\"tbd@tbd.com\"]"
          - name: SCAN_EMAIL_SUBJECT
            value: "Security Overview"
          - name: SCAN_S3_PUBLIC_BUCKET
            value: "public-insights"
          - name: SCAN_S3_PRIVATE_BUCKET
            value: "insights"
          - name: SCAN_GA_ID
            value: "UA-000000000-0"
          - name: SCAN_COMPANY_NAME
            value: "Example"
          - name: SCAN_SUPPORT_EMAIL
            value: "vulcan@example.com"
          - name: SCAN_CONTACT_EMAIL
            value: "vulcan@example.com"
          - name: SCAN_CONTACT_CHANNEL
            value: "https://example.slack.com/archives/XXXXX"
          - name: SCAN_CONTACT_JIRA
            value: "https://jira.example.com/"
          - name: SCAN_DOCS_API_LINK
            value: "https://docs.example.com/vulcan/vulcan-api/examples/#how-do-i-list-the-members-of-a-team"
          - name: SCAN_DOCS_ROADMAP_LINK
            value: "https://docs.example.com/vulcan/roadmap"
          - name: PERSISTENCE_ENDPOINT  # We keep this PERSISTENCE variable for compatibility
            value: "http://myrelease-vulcan-scanengine"
          - name: RESULTS_ENDPOINT
            value: "http://myrelease-vulcan-results"
          - name: SCAN_PROXY_ENDPOINT
            value: "http://insights.vulcan.local"
          - name: VULCAN_UI
            value: "http://www.vulcan.local/"
          - name: SCAN_VIEW_REPORT
            value: "http://www.vulcan.local/api/v1/report?team_id=%s&scan_id=%s"
          - name: LIVEREPORT_EMAIL_SUBJECT
            value: 
          
          - name: AWS_SQS_ENDPOINT
            value: "http://myrelease-vulcan-goaws"
          - name: AWS_ACCESS_KEY_ID
            value: ANYVALUE
          - name: AWS_SECRET_ACCESS_KEY
            value: ANYVALUE
          - name: DOGSTATSD_ENABLED
            value: "true"
          - name: DOGSTATSD_HOST
            value: "localhost"
          - name: DOGSTATSD_PORT
            value: "8125"
          envFrom:
          - secretRef:
              name: myrelease-vulcan-reportsgenerator
          ports:
            - name: app
              containerPort: 8080
              protocol: TCP
      volumes:
      - name: config-proxy
        configMap:
          name: myrelease-vulcan-reportsgenerator-proxy
---
# Source: vulcan/templates/results/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myrelease-vulcan-results
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: results
spec:
  selector:
    matchLabels:
      app.kubernetes.io/instance: vulcan
      app.kubernetes.io/name: results
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: vulcan
        app.kubernetes.io/name: results
      annotations:
        checksum/config-proxy: 10917d04d8ae06d9d5b1d003e8188d929eda9c6e3fd09ed7fde464f9825698bc
        prometheus.io/scrape: 'true'
        prometheus.io/port: '9101'
    spec:
      containers:
        
        - name: dogstatsd
          image: "datadog/dogstatsd:7.37.1"
          envFrom:
          - secretRef:
              name: myrelease-vulcan-dogstatsd
          ports:
            - containerPort: 8125
              name: dogstatsd
              protocol: UDP
        - name: proxy
          image: "haproxy:2.4.17-alpine3.16"
          imagePullPolicy: Always
          ports:
            - name: http
              containerPort: 9090
            - name: metrics
              containerPort: 9101
          volumeMounts:
          - mountPath: /usr/local/etc/haproxy
            readOnly: true
            name: config-proxy
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh","-c","sleep 30;"]
        - name: results
          
          image: "adevinta/vulcan-results:1.0"
          imagePullPolicy: Always
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh","-c","sleep 30;"]
          livenessProbe:
            httpGet:
              path: /healthcheck
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 10
          readinessProbe:
            httpGet:
              path: /healthcheck
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 5
          env:
          - name: PORT
            value: "8080"
          - name: DEBUG
            value: "false"
          - name: AWS_REGION
            value: "local"
          - name: BUCKET_REPORTS
            value: "reports"
          - name: BUCKET_LOGS
            value: "logs"
          - name: LINK_BASE
            value: "https://results.vulcan.local/v1"
          
          - name: AWS_S3_ENDPOINT
            value: "http://myrelease-minio"
          - name: PATH_STYLE
            value: "true"
          - name: AWS_S3_REGION
            value: "local"
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: myrelease-minio
                key: root-user
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: myrelease-minio
                key: root-password
          - name: DOGSTATSD_ENABLED
            value: "true"
          - name: DOGSTATSD_HOST
            value: "localhost"
          - name: DOGSTATSD_PORT
            value: "8125"
          ports:
            - name: app
              containerPort: 8080
              protocol: TCP
      volumes:
      - name: config-proxy
        configMap:
          name: myrelease-vulcan-results-proxy
---
# Source: vulcan/templates/scanengine/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myrelease-vulcan-scanengine
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: scanengine
spec:
  selector:
    matchLabels:
      app.kubernetes.io/instance: vulcan
      app.kubernetes.io/name: scanengine
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: vulcan
        app.kubernetes.io/name: scanengine
      annotations:
        checksum/secrets: 6a98d86e2647d0c9b6bb9403aaf5a023ea894e2e305370d3d147cf9a60986fc9
        checksum/config-proxy: 661738336e8ae7617f1a7a5110f36024f9cff3df2effcfd62cc5faa93d5e8909
        prometheus.io/scrape: 'true'
        prometheus.io/port: '9101'
    spec:
      initContainers:
        - name: waitfordb
          image: "busybox:1.35.0"
          imagePullPolicy: Always
          command: ['sh', '-c', 'until nc -z "$PGHOST" "$PGPORT"; do echo WaitingDB && sleep 5; done;']
          env:
          - name: PGHOST
            value: "myrelease-postgresql"
          - name: PGPORT
            value: "5432"
      containers:
        
        - name: dogstatsd
          image: "datadog/dogstatsd:7.37.1"
          envFrom:
          - secretRef:
              name: myrelease-vulcan-dogstatsd
          ports:
            - containerPort: 8125
              name: dogstatsd
              protocol: UDP
        - name: proxy
          image: "haproxy:2.4.17-alpine3.16"
          imagePullPolicy: Always
          ports:
            - name: http
              containerPort: 9090
            - name: metrics
              containerPort: 9101
          volumeMounts:
          - mountPath: /usr/local/etc/haproxy
            readOnly: true
            name: config-proxy
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh","-c","sleep 30;"]
        - name: scanengine
          
          image: "adevinta/vulcan-scan-engine:1.0"
          imagePullPolicy: Always
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh","-c","sleep 30;"]
          livenessProbe:
            httpGet:
              path: /v1/healthcheck
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 10
          readinessProbe:
            httpGet:
              path: /v1/healthcheck
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 5
          env:
          - name: PORT
            value: "8080"
          - name: PG_HOST
            value: "myrelease-postgresql"
          - name: PG_NAME
            value: "scanengine"
          - name: PG_USER
            value: "postgres"
          - name: PG_PORT
            value: "5432"
          - name: PG_SSLMODE
            value: "disable"
          - name: PG_CA_B64
            value: ""
          - name: LOG_LEVEL
            value: "error"
          - name: PERSISTENCE_HOST
            value: "myrelease-vulcan-persistence"
          - name: CHECKS_SQS_ARN
            value: "arn:aws:sqs:local:012345678900:VulcanK8SScanEngineCheckStatus"
          - name: SCANS_SNS_ARN
            value: "arn:aws:sns:local:012345678900:VulcanK8SScans"
          - name: CHECKS_SNS_ARN
            value: "arn:aws:sns:local:012345678900:VulcanK8SChecks"
          - name: STREAM_URL
            value: "http://myrelease-vulcan-stream"
          - name: CHECKS_CREATOR_WORKERS
            value: "2"
          - name: CHECKS_CREATOR_PERIOD
            value: "20"
          - name: QUEUES_DEFAULT_ARN
            value: "arn:aws:sqs:local:012345678900:VulcanK8SV2ChecksGeneric"
          - name: AWS_SNS_ENDPOINT
            value: "http://myrelease-vulcan-goaws"
          - name: AWS_SQS_ENDPOINT
            value: "http://myrelease-vulcan-goaws"
          - name: AWS_ACCESS_KEY_ID
            value: ANYVALUE
          - name: AWS_SECRET_ACCESS_KEY
            value: ANYVALUE
          - name: DOGSTATSD_ENABLED
            value: "true"
          - name: DOGSTATSD_HOST
            value: "localhost"
          - name: DOGSTATSD_PORT
            value: "8125"
          envFrom:
          - secretRef:
              name: myrelease-vulcan-scanengine
          ports:
            - name: app
              containerPort: 8080
              protocol: TCP
      volumes:
      - name: config-proxy
        configMap:
          name: myrelease-vulcan-scanengine-proxy
---
# Source: vulcan/templates/sqsexporter/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myrelease-vulcan-sqsexporter
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: sqsexporter
    app.kubernetes.io/name: sqsexporter
spec:
  selector:
    matchLabels:
      app.kubernetes.io/instance: vulcan
      app.kubernetes.io/name: sqsexporter
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: vulcan
        app.kubernetes.io/name: sqsexporter
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: "8080"
    spec:
      containers:
        - name: sqsexporter
          
          image: "jesusfcr/sqs-prometheus-exporter:0.4.0"
          imagePullPolicy: Always
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh","-c","sleep 30;"]
          env:
          - name: PORT
            value: "8080"
          - name: SQS_QUEUE_NAME_PREFIX
            value: VulcanK8S
          - name: AWS_REGION
            value: "local"
          
          - name: AWS_SQS_ENDPOINT
            value: "http://myrelease-vulcan-goaws"
          - name: AWS_ACCESS_KEY_ID
            value: ANYVALUE
          - name: AWS_SECRET_ACCESS_KEY
            value: ANYVALUE
          
          ports:
            - name: metrics
              containerPort: 8080
              protocol: TCP
---
# Source: vulcan/templates/stream/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myrelease-vulcan-stream
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: stream
spec:
  selector:
    matchLabels:
      app.kubernetes.io/instance: vulcan
      app.kubernetes.io/name: stream
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: vulcan
        app.kubernetes.io/name: stream
      annotations:
        checksum/config-proxy: 227c8578a7f5395fd4b9bb71c16234b2842110eacb5a717f76e13dc9e0fc4a3c
        prometheus.io/scrape: 'true'
        prometheus.io/port: '9101'
    spec:
      containers:
        
        - name: dogstatsd
          image: "datadog/dogstatsd:7.37.1"
          envFrom:
          - secretRef:
              name: myrelease-vulcan-dogstatsd
          ports:
            - containerPort: 8125
              name: dogstatsd
              protocol: UDP
        - name: proxy
          image: "haproxy:2.4.17-alpine3.16"
          imagePullPolicy: Always
          ports:
            - name: http
              containerPort: 9090
            - name: metrics
              containerPort: 9101
          volumeMounts:
          - mountPath: /usr/local/etc/haproxy
            readOnly: true
            name: config-proxy
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh","-c","sleep 30;"]
        - name: stream
          
          image: "adevinta/vulcan-stream:1.0"
          imagePullPolicy: Always
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh","-c","sleep 30;"]
          livenessProbe:
            httpGet:
              path: /status
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 10
          readinessProbe:
            httpGet:
              path: /status
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 5
          env:
          - name: PORT
            value: "8080"
          - name: LOG_LEVEL
            value: "DEBUG"
          - name: REDIS_HOST
            value: "myrelease-redis-master"
          - name: REDIS_USR
            value: ""
          - name: REDIS_PORT
            value: "6379"
          - name: REDIS_DB
            value: "0"
          - name: REDIS_TTL
            value: "0"
          
          - name: DOGSTATSD_ENABLED
            value: "true"
          - name: DOGSTATSD_HOST
            value: "localhost"
          - name: DOGSTATSD_PORT
            value: "8125"
          ports:
            - name: app
              containerPort: 8080
              protocol: TCP
      volumes:
      - name: config-proxy
        configMap:
          name: myrelease-vulcan-stream-proxy
---
# Source: vulcan/templates/ui/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myrelease-vulcan-ui
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ui
spec:
  selector:
    matchLabels:
      app.kubernetes.io/instance: vulcan
      app.kubernetes.io/name: ui
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: vulcan
        app.kubernetes.io/name: ui
      annotations:
        checksum/config-proxy: e3037ec88156eaebef76365dca92abef2e50bb4f5de0eb5313074032c762ce3a
        prometheus.io/scrape: 'true'
        prometheus.io/port: '9101'
    spec:
      containers:
        
        - name: proxy
          image: "haproxy:2.4.17-alpine3.16"
          imagePullPolicy: Always
          ports:
            - name: http
              containerPort: 9090
            - name: metrics
              containerPort: 9101
          volumeMounts:
          - mountPath: /usr/local/etc/haproxy
            readOnly: true
            name: config-proxy
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh","-c","sleep 30;"]
        - name: ui
          
          image: "adevinta/vulcan-ui:1.0"
          imagePullPolicy: Always
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh","-c","sleep 30;"]
          livenessProbe:
            httpGet:
              path: /index.html
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 10
          readinessProbe:
            httpGet:
              path: /index.html
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 5
          env:
          - name: PORT
            value: "8080"
          - name: API_URL
            value: "https://www.vulcan.local/api/v1/"
          - name: UI_DOCS_API_LINK
            value: "https://docs.example.com/vulcan/vulcan-api/"
          - name: UI_DOCS_WHITELISTING_LINK
            value: 
          - name: UI_DOCS_DISCOVERY_LINK
            value: "https://docs.example.com/vulcan/discovery"
          - name: UI_DOCS_AUDITROLE_LINK
            value: "https://docs.example.com/vulcan/network-access"
          - name: UI_CONTACT_EMAIL
            value: "vulcan@example.com"
          - name: UI_CONTACT_SLACK
            value: "https://example.slack.com/archives/XXXXX"
          - name: UI_DASHBOARD_LINK
            value: 
          
          
          ports:
            - name: app
              containerPort: 8080
              protocol: TCP
      volumes:
      - name: config-proxy
        configMap:
          name: myrelease-vulcan-ui-proxy
---
# Source: vulcan/templates/vulndb/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myrelease-vulcan-vulndb
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: vulndb
spec:
  selector:
    matchLabels:
      app.kubernetes.io/instance: vulcan
      app.kubernetes.io/name: vulndb
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: vulcan
        app.kubernetes.io/name: vulndb
      annotations:
        checksum/secrets: ff76b6caa4d4af25d5701d0fe380652c74ea9038f2eb3644af242192d8f68fc0
        
    spec:
      initContainers:
        - name: waitfordb
          image: "busybox:1.35.0"
          imagePullPolicy: Always
          command: ['sh', '-c', 'until nc -z "$PGHOST" "$PGPORT"; do echo WaitingDB && sleep 5; done;']
          env:
          - name: PGHOST
            value: "myrelease-postgresql"
          - name: PGPORT
            value: "5432"
      containers:
        
        - name: vulndb
          
          image: "adevinta/vulnerability-db:1.0"
          imagePullPolicy: Always
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh","-c","sleep 30;"]
          env:
          - name: PG_HOST
            value: "myrelease-postgresql"
          - name: PG_NAME
            value: "vulnerabilitydb"
          - name: PG_USER
            value: "postgres"
          - name: PG_PORT
            value: "5432"
          - name: PG_SSLMODE
            value: "disable"
          - name: PG_CA_B64
            value: ""
          - name: LOG_LEVEL
            value: "error"
          - name: MAX_EVENT_AGE
            value: "365"
          - name: SQS_QUEUE_ARN
            value: "arn:aws:sqs:local:012345678900:VulcanK8SVulnDBChecks"
          - name: SNS_TOPIC_ARN
            value: "arn:aws:sns:local:012345678900:VulcanK8SVulnDBVulns"
          - name: RESULTS_URL
            value: http://vulcan-results.vulcan.com
          - name: RESULTS_INTERNAL_URL
            value: "http://myrelease-vulcan-results"
          - name: AWS_SNS_ENDPOINT
            value: "http://myrelease-vulcan-goaws"
          - name: AWS_SQS_ENDPOINT
            value: "http://myrelease-vulcan-goaws"
          - name: AWS_ACCESS_KEY_ID
            value: ANYVALUE
          - name: AWS_SECRET_ACCESS_KEY
            value: ANYVALUE
          
          envFrom:
          - secretRef:
              name: myrelease-vulcan-vulndb
      volumes:
---
# Source: vulcan/templates/vulndbapi/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myrelease-vulcan-vulndbapi
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: vulndbapi
spec:
  selector:
    matchLabels:
      app.kubernetes.io/instance: vulcan
      app.kubernetes.io/name: vulndbapi
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: vulcan
        app.kubernetes.io/name: vulndbapi
      annotations:
        checksum/secrets: ff76b6caa4d4af25d5701d0fe380652c74ea9038f2eb3644af242192d8f68fc0
        checksum/config-proxy: ac9c854e845928f39557c9b2728c0438304971c28a0ba55e85507af9caf37222
        prometheus.io/scrape: 'true'
        prometheus.io/port: '9101'
    spec:
      initContainers:
        - name: waitfordb
          image: "busybox:1.35.0"
          imagePullPolicy: Always
          command: ['sh', '-c', 'until nc -z "$PGHOST" "$PGPORT"; do echo WaitingDB && sleep 5; done;']
          env:
          - name: PGHOST
            value: "myrelease-postgresql"
          - name: PGPORT
            value: "5432"
      containers:
        
        - name: proxy
          image: "haproxy:2.4.17-alpine3.16"
          imagePullPolicy: Always
          ports:
            - name: http
              containerPort: 9090
            - name: metrics
              containerPort: 9101
          volumeMounts:
          - mountPath: /usr/local/etc/haproxy
            readOnly: true
            name: config-proxy
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh","-c","sleep 30;"]
        - name: vulndbapi
          
          image: "adevinta/vulnerability-db-api:1.0"
          imagePullPolicy: Always
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh","-c","sleep 30;"]
          livenessProbe:
            httpGet:
              path: /healthcheck
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 10
          readinessProbe:
            httpGet:
              path: /healthcheck
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 5
          env:
          - name: PORT
            value: "8080"
          - name: PG_HOST
            value: "myrelease-postgresql"
          - name: PG_NAME
            value: "vulnerabilitydb"
          - name: PG_USER
            value: "postgres"
          - name: PG_PORT
            value: "5432"
          - name: PG_SSLMODE
            value: "disable"
          - name: PG_CA_B64
            value: ""
          - name: LOG_LEVEL
            value: "info"
          
          
          envFrom:
          - secretRef:
              name: myrelease-vulcan-vulndbapi
          ports:
            - name: app
              containerPort: 8080
              protocol: TCP
      volumes:
      - name: config-proxy
        configMap:
          name: myrelease-vulcan-vulndbapi-proxy
---
# Source: vulcan/charts/postgresql/templates/primary/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: myrelease-postgresql
  namespace: "ns"
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-11.6.6
    app.kubernetes.io/instance: myrelease
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
  annotations:
    prometheus.io/port: "9187"
    prometheus.io/scrape: "true"
spec:
  replicas: 1
  serviceName: myrelease-postgresql-hl
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/instance: myrelease
      app.kubernetes.io/component: primary
  template:
    metadata:
      name: myrelease-postgresql
      labels:
        app.kubernetes.io/name: postgresql
        helm.sh/chart: postgresql-11.6.6
        app.kubernetes.io/instance: myrelease
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: primary
      annotations:
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: postgresql
                    app.kubernetes.io/instance: myrelease
                    app.kubernetes.io/component: primary
                namespaces:
                  - "ns"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      hostNetwork: false
      hostIPC: false
      initContainers:
      containers:
        - name: postgresql
          image: docker.io/bitnami/postgresql:14.3.0-debian-11-r3
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: "/bitnami/postgresql"
            - name: PGDATA
              value: "/bitnami/postgresql/data"
            # Authentication
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: myrelease-postgresql
                  key: postgres-password
            - name: POSTGRES_DB
              value: "persistence"
            # Replication
            # Initdb
            # Standby
            # LDAP
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
            # TLS
            - name: POSTGRESQL_ENABLE_TLS
              value: "no"
            # Audit
            - name: POSTGRESQL_LOG_HOSTNAME
              value: "false"
            - name: POSTGRESQL_LOG_CONNECTIONS
              value: "false"
            - name: POSTGRESQL_LOG_DISCONNECTIONS
              value: "false"
            - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
              value: "off"
            # Others
            - name: POSTGRESQL_CLIENT_MIN_MESSAGES
              value: "error"
            - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
              value: "pgaudit"
          ports:
            - name: tcp-postgresql
              containerPort: 5432
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "postgres" -d "dbname=persistence" -h 127.0.0.1 -p 5432
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                
                - |
                  exec pg_isready -U "postgres" -d "dbname=persistence" -h 127.0.0.1 -p 5432
                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
          resources:
            limits: {}
            requests:
              cpu: "0"
              memory: "0"
          volumeMounts:
            - name: custom-init-scripts
              mountPath: /docker-entrypoint-initdb.d/
            - name: dshm
              mountPath: /dev/shm
        - name: metrics
          image: docker.io/bitnami/postgres-exporter:0.10.1-debian-11-r3
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: DATA_SOURCE_URI
              value: 127.0.0.1:5432/persistence?sslmode=disable
            - name: DATA_SOURCE_PASS
              valueFrom:
                secretKeyRef:
                  name: myrelease-postgresql
                  key: postgres-password
            - name: DATA_SOURCE_USER
              value: "postgres"
          ports:
            - name: http-metrics
              containerPort: 9187
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /
              port: http-metrics
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /
              port: http-metrics
          volumeMounts:
          resources:
            limits: {}
            requests: {}
      volumes:
        - name: custom-init-scripts
          configMap:
            name: myrelease-postgresql-init-scripts
        - name: dshm
          emptyDir:
            medium: Memory
        - name: data
          emptyDir: {}
---
# Source: vulcan/charts/redis/templates/master/application.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: myrelease-redis-master
  namespace: "ns"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-16.12.1
    app.kubernetes.io/instance: myrelease
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: master
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: redis
      app.kubernetes.io/instance: myrelease
      app.kubernetes.io/component: master
  serviceName: myrelease-redis-headless
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: redis
        helm.sh/chart: redis-16.12.1
        app.kubernetes.io/instance: myrelease
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: master
      annotations:
        checksum/configmap: 485b809d3bc5a3f1c0b68df8f1d8fe15cae987cfea33e00cfa0b1eb75f20d4b4
        checksum/health: 8aa41f5591c813adbca49b317cbbc963b957a1844bf22b642bfc3c63dcff7524
        checksum/scripts: fc3eb572f421f20a10a52dbb949eaa3518caeec741bae56fb918eee3aec78f42
        checksum/secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
    spec:
      
      securityContext:
        fsGroup: 1001
      serviceAccountName: default
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: redis
                    app.kubernetes.io/instance: myrelease
                    app.kubernetes.io/component: master
                namespaces:
                  - "ns"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      terminationGracePeriodSeconds: 30
      containers:
        - name: redis
          image: docker.io/bitnami/redis:6.2.7-debian-11-r3
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          command:
            - /bin/bash
          args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-master.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: master
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          ports:
            - name: redis
              containerPort: 6379
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            # One second longer than command timeout should prevent generation of zombie processes.
            timeoutSeconds: 6
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local.sh 5
          readinessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 2
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local.sh 1
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
              subPath: 
            - name: config
              mountPath: /opt/bitnami/redis/mounted-etc
            - name: redis-tmp-conf
              mountPath: /opt/bitnami/redis/etc/
            - name: tmp
              mountPath: /tmp
      volumes:
        - name: start-scripts
          configMap:
            name: myrelease-redis-scripts
            defaultMode: 0755
        - name: health
          configMap:
            name: myrelease-redis-health
            defaultMode: 0755
        - name: config
          configMap:
            name: myrelease-redis-configuration
        - name: redis-tmp-conf
          emptyDir: {}
        - name: tmp
          emptyDir: {}
        - name: redis-data
          emptyDir: {}
---
# Source: vulcan/charts/minio/templates/ingress.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: myrelease-minio
  namespace: "ns"
  labels:
    app.kubernetes.io/name: minio
    helm.sh/chart: minio-11.7.4
    app.kubernetes.io/instance: myrelease
    app.kubernetes.io/managed-by: Helm
  annotations:
spec:
  rules:
    - host: minio.vulcan.local
      http:
        paths:
          - path: /
            backend:
              serviceName: myrelease-minio
              servicePort: minio-console
---
# Source: vulcan/templates/api/ingress.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: myrelease-vulcan-api
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: api
  annotations:
    nginx.ingress.kubernetes.io/cors-allow-origin: https://www.vulcan.local
    nginx.ingress.kubernetes.io/enable-cors: "true"
    nginx.ingress.kubernetes.io/proxy-body-size: 8m
spec:
  rules:
    - host: "www.vulcan.local"
      http:
        paths:
          - path: /api
            backend:
              serviceName: myrelease-vulcan-api
              servicePort: 80
---
# Source: vulcan/templates/goaws/ingress.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: myrelease-vulcan-goaws
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: goaws
spec:
  rules:
    - host: "goaws.vulcan.local"
      http:
        paths:
          - path: /
            backend:
              serviceName: myrelease-vulcan-goaws
              servicePort: 80
---
# Source: vulcan/templates/insights/ingress.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: myrelease-vulcan-insights
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: insights
  annotations:
    nginx.ingress.kubernetes.io/configuration-snippet: |
      more_set_headers "X-Frame-Options: SAMEORIGIN";
      more_set_headers "X-Content-Type-Options: nosniff";
      more_set_headers "X-Frame-Options: DENY";
      more_set_headers "X-Xss-Protection: 1";
      more_set_headers "Strict-Transport-Security: max-age=31536000; includeSubDomains";
      more_set_headers "Content-Security-Policy: default-src 'none'; script-src 'self' 'unsafe-inline' https://insights.vulcan.local https://www.google-analytics.com; font-src 'self' https://insights.vulcan.local; connect-src 'self' https://insights.vulcan.local; img-src 'self' https://insights.vulcan.local https://www.google-analytics.com; style-src 'self' 'unsafe-inline' https://insights.vulcan.local; object-src 'none'";
spec:
  rules:
    - host: "insights.vulcan.local"
      http:
        paths:
          - path: /
            backend:
              serviceName: myrelease-vulcan-insights
              servicePort: 80
---
# Source: vulcan/templates/persistence/ingress.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: myrelease-vulcan-persistence
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: persistence
spec:
  rules:
    - host: "persistence.vulcan.local"
      http:
        paths:
          - path: /
            backend:
              serviceName: myrelease-vulcan-persistence
              servicePort: 80
---
# Source: vulcan/templates/results/ingress.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: myrelease-vulcan-results
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: results
spec:
  rules:
    - host: "results.vulcan.local"
      http:
        paths:
          - path: /
            backend:
              serviceName: myrelease-vulcan-results
              servicePort: 80
---
# Source: vulcan/templates/stream/ingress.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: myrelease-vulcan-stream
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: stream
  annotations:
    nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"
spec:
  rules:
    - host: "stream.vulcan.local"
      http:
        paths:
          - path: /
            backend:
              serviceName: myrelease-vulcan-stream
              servicePort: 80
---
# Source: vulcan/templates/ui/ingress.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: myrelease-vulcan-ui
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ui
spec:
  rules:
    - host: "www.vulcan.local"
      http:
        paths:
          - path: /
            backend:
              serviceName: myrelease-vulcan-ui
              servicePort: 80
---
# Source: vulcan/templates/vulndbapi/ingress.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: myrelease-vulcan-vulndbapi
  labels:
    helm.sh/chart: vulcan-0.4.1
    app.kubernetes.io/instance: vulcan
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: vulndbapi
spec:
  rules:
    - host: "vulndbapi.vulcan.local"
      http:
        paths:
          - path: /
            backend:
              serviceName: myrelease-vulcan-vulndbapi
              servicePort: 80
